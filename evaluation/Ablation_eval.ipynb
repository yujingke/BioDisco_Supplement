{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21df57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly evaluate: score biomedical hypotheses on four metrics (novelty, relevance, significance, verifiability), outputting results to CSV.\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import backoff\n",
    "import re\n",
    "from statistics import mode, mean\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_models(model_list=[\"gpt-4.1\"])\n",
    "gpt_config = {\n",
    "    \"chat_model\": \"gpt-4.1\",\n",
    "    \"cache_seed\": 42,\n",
    "    \"temperature\": 0.7,\n",
    "    \"config_list\": config_list,\n",
    "    \"timeout\": 540000,\n",
    "    \"max_output_tokens\": 1500\n",
    "}\n",
    "\n",
    "# Logger\n",
    "logger = logging.getLogger(__name__)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "METRICS_PROMPT = r\"\"\"\n",
    "You are a senior biomedical reviewer.\n",
    "\n",
    "Task:\n",
    "Evaluate the following hypothesis by assigning a score for each metric (Novelty, Relevance, Significance, Verifiability) and providing a concise reason (<=15 words).\n",
    "\n",
    "Metric definitions:\n",
    "Novelty: Evaluate the novelty of the generated scientific hypothesis. The score range should be 0 to 3. 0 means there's no novelty, which indicates that the hypothesis is a paraphrase of the input. 1 means there's slight novelty. 2 means there's moderate novelty. 3 means the hypothesis has strong novelty, which gives new insights beyond the background. Output is an integer.\n",
    "Relevance: Evaluate the relevance of the generated scientific hypothesis. The score range should be 0 to 3. 0 means there's no relevance. 1 means there's slight relevance. 2 means there's moderate relevance. 3 means they are strongly related. Output is an integer.\n",
    "Significance: Evaluate the significance of the generated scientific hypothesis. The score range should be 0 to 3. 0 means there's no significance, which indicates that the hypothesis is just a common knowledge. 1 means there's slight significance. 2 means there's moderate significance. 3 means the hypothesis has strong significance, which gives significant insights beyond the background. Output is an integer.\n",
    "Verifiability: Evaluate the verifiability of the generated scientific hypothesis. The score range should be 0 to 3. 0 means there's no verifiability, which indicates that the hypothesis is not possible to be verified in future work. 1 means there's slight verifiability. 2 means there's moderate verifiability. 3 means the hypothesis has strong verifiability, which means the hypothesis is very likely to be verified in future work. Output is an integer.\n",
    "\n",
    "Return format:\n",
    "{{\n",
    "  \"novelty\":{{\"score\":<float>,\"reason\":\"≤15 words\"}},\n",
    "  \"relevance\":{{\"score\":<float>,\"reason\":\"≤15 words\"}},\n",
    "  \"significance\":{{\"score\":<float>,\"reason\":\"≤15 words\"}},\n",
    "  \"verifiability\":{{\"score\":<float>,\"reason\":\"≤15 words\"}}\n",
    "}}\n",
    "\n",
    "Background: {background}\n",
    "\n",
    "Hypothesis: {hypothesis}\n",
    "\"\"\"\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.OpenAIError, max_time=90)\n",
    "def call_openai_metrics(bg: str, hp: str, samples: int) -> list:\n",
    "    prompt = METRICS_PROMPT.format(background=bg, hypothesis=hp)\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=gpt_config[\"chat_model\"],\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Use half-point scale.\"},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        temperature=gpt_config[\"temperature\"],\n",
    "        max_tokens=gpt_config[\"max_output_tokens\"],\n",
    "        n=samples\n",
    "    )\n",
    "    out = []\n",
    "    for c in resp.choices:\n",
    "        txt = c.message.content\n",
    "        txt = re.sub(r\"```(?:json)?\", \"\", txt).strip() # type: ignore\n",
    "        try:\n",
    "            out.append(json.loads(txt))\n",
    "        except:\n",
    "            logger.warning(\"Still bad metrics JSON: %s\", txt[:120])\n",
    "    return out\n",
    "\n",
    "def aggregate_metrics(samples: list) -> dict:\n",
    "    final = {}\n",
    "    for m in [\"novelty\", \"relevance\", \"significance\", \"verifiability\"]:\n",
    "        vals = [s[m][\"score\"] for s in samples\n",
    "                if isinstance(s.get(m), dict) and isinstance(s[m].get(\"score\"), (int, float))]\n",
    "        if not vals:\n",
    "            final[m] = None\n",
    "        else:\n",
    "            sc = mode(vals) if vals and vals.count(mode(vals)) > 1 else round(mean(vals) * 2) / 2\n",
    "            final[m] = sc\n",
    "    return final\n",
    "\n",
    "def load_entries_multi(path: Path):\n",
    "    if not path.exists(): \n",
    "        print(f\"file {path} Nothing exists\")\n",
    "        return []\n",
    "    out = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                o = json.loads(line)\n",
    "                disease = o.get(\"disease\", \"\")\n",
    "                core_genes = o.get(\"core_genes\", [])\n",
    "                background = f\"Disease: {disease}\\nCore genes: {', '.join(core_genes)}\"\n",
    "                hypos = o.get(\"hypotheses\", [])\n",
    "                for hypo in hypos:\n",
    "                    h = hypo.strip()\n",
    "                    if h and background:\n",
    "                        out.append((background, h))\n",
    "            except Exception as e:\n",
    "                print(\"Line parsing error:\", e)\n",
    "    print(f\"Successfully generated {len(out)} hypothesis records\")\n",
    "    return out\n",
    "\n",
    "input_path = Path(r\"../data/data_raw/raw_baseline.jsonl\")\n",
    "output_path = Path(\"direct_baseline.csv\")\n",
    "samples = 3\n",
    "\n",
    "entries = load_entries_multi(input_path)\n",
    "\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"index\", \"novelty\", \"relevance\", \"significance\", \"verifiability\"])\n",
    "    for idx, (bg, hp) in enumerate(entries, 1):\n",
    "        m = aggregate_metrics(call_openai_metrics(bg, hp, samples))\n",
    "        row = [idx, m[\"novelty\"], m[\"relevance\"], m[\"significance\"], m[\"verifiability\"]]\n",
    "        writer.writerow(row)\n",
    "        f.flush()  \n",
    "        logger.info(f\"[{idx}] Novelty={m['novelty']} | Relevance={m['relevance']} | Significance={m['significance']} | Verifiability={m['verifiability']}\")\n",
    "\n",
    "logger.info(f\"Saved {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0046ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise evaluation of two biomedical hypotheses on four metrics (novelty, relevance, significance, verifiability); outputs comparison results for A vs. B.\n",
    "import json\n",
    "import openai\n",
    "import backoff\n",
    "import re\n",
    "import logging\n",
    "\n",
    "import autogen\n",
    "try:\n",
    "    config_list = autogen.config_list_from_models(model_list=[\"gpt-4.1\"])\n",
    "except Exception:\n",
    "    config_list = []\n",
    "\n",
    "gpt_config = {\n",
    "    \"model_name\":        \"gpt-4.1\",\n",
    "    \"temperature\":       0.2,\n",
    "    \"top_p\":             1.0,\n",
    "    \"max_tokens\":        400,\n",
    "    \"cache_seed\":        42,\n",
    "    \"config_list\":       config_list,\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.handlers.clear()\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "metrics = [\"Novelty\", \"Relevance\", \"Significance\", \"Verifiability\"]\n",
    "metric_defs = [\n",
    "    # Novelty\n",
    "    \"\"\"\n",
    "You are an expert in biomedicine.\n",
    "\n",
    "Evaluate the novelty of two scientific hypotheses (H_A and H_B) given the user input.\n",
    "\n",
    "For each, assign a novelty score from 0 to 3. 0 means there's no novelty, which indicates that the hypothesis is a paraphrase of the background. 1 means there's slight novelty. 2 means there's moderate novelty. 3 means the hypothesis has strong novelty, which gives new insights beyond the background. Score two hypotheses and compare which one is more novel(“A”, “B”, or “0” if equal or difference is unclear) \n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    # Relevance\n",
    "    \"\"\"\n",
    " You are an expert in biomedicine.\n",
    "\n",
    "Evaluate the relevance of two scientific hypotheses (H_A and H_B) given the user input.\n",
    "\n",
    "For each, assign a relevance score from 0 to 3. 0 means there's no relevance. 1 means there's slight relevance. 2 means there's moderate relevance. 3 means the hypothesis is strongly related to the background. Score both hypotheses and compare which one is more relevant (“A”, “B”, or “0” if equal or difference is unclear)\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    # Significance\n",
    "    \"\"\"\n",
    "You are an expert in biomedicine.\n",
    "\n",
    "Evaluate the significance of two scientific hypotheses (H_A and H_B) given the user input.\n",
    "\n",
    "For each, assign a significance score from 0 to 3. 0 means there's no significance, which indicates that the hypothesis is just common knowledge. 1 means there's slight significance. 2 means there's moderate significance. 3 means the hypothesis has strong significance, providing significant insights beyond the background. Score both hypotheses and compare which one is more significant (“A”, “B”, or “0” if equal or difference is unclear)\n",
    "\n",
    "\"\"\",\n",
    "\n",
    "    # Verifiability\n",
    "    \"\"\"\n",
    "You are an expert in biomedicine.\n",
    "\n",
    "Evaluate the verifiability of two scientific hypotheses (H_A and H_B) given the user input.\n",
    "\n",
    "For each, assign a verifiability score from 0 to 3. 0 means there's no verifiability, which indicates that the hypothesis is not possible to be verified in future work. 1 means there's slight verifiability. 2 means there's moderate verifiability. 3 means the hypothesis has strong verifiability, which means it is very likely to be verified in future work. Score both hypotheses and compare which one is more verifiable (“A”, “B”, or “0” if equal or difference is unclear)\n",
    "\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.OpenAIError, max_time=60)\n",
    "def call_all_metrics(bg: str, h_a: str, h_b: str, metrics: list, metric_defs: list):\n",
    "    SYSTEM_METRIC = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "You are a senior biomedical reviewer. Compare two hypotheses H_A and H_B on **four metrics**: Novelty, Relevance, Significance, Verifiability.\n",
    "\n",
    "Instructions:\n",
    "- For each metric, judge and select a winner:\n",
    "    - \"A\" if H_A is clearly superior,\n",
    "    - \"B\" if H_B is clearly superior,\n",
    "    - \"0\" if they are equal or difference is unclear.\n",
    "- For each, give a concise reason .\n",
    "- Each metric is judged strictly independently.\n",
    "\n",
    "Definitions:\n",
    "Novelty: {metric_defs[0]}\n",
    "Relevance: {metric_defs[1]}\n",
    "Significance: {metric_defs[2]}\n",
    "Verifiability: {metric_defs[3]}\n",
    "\n",
    "Return **exactly one JSON** object:\n",
    "{{\n",
    "  \"Novelty\": {{\"winner\": \"A\"|\"B\"|\"0\", \"reason\": \"...\" }},\n",
    "  \"Relevance\": {{\"winner\": \"A\"|\"B\"|\"0\", \"reason\": \"...\" }},\n",
    "  \"Significance\": {{\"winner\": \"A\"|\"B\"|\"0\", \"reason\": \"...\" }},\n",
    "  \"Verifiability\": {{\"winner\": \"A\"|\"B\"|\"0\", \"reason\": \"...\" }}\n",
    "}}\n",
    "No extra explanation.\n",
    "\"\"\".strip()\n",
    "    }\n",
    "    user = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"User Input: {bg}\\n\\nH_A: {h_a}\\nH_B: {h_b}\"\n",
    "    }\n",
    "    resp = openai.chat.completions.create(\n",
    "        model       = gpt_config[\"model_name\"],\n",
    "        messages    = [SYSTEM_METRIC, user], # type: ignore\n",
    "        temperature = gpt_config[\"temperature\"],\n",
    "        top_p       = gpt_config[\"top_p\"],\n",
    "        max_tokens  = 400\n",
    "    )\n",
    "    txt = resp.choices[0].message.content or \"\"\n",
    "    txt = re.sub(r\"```(?:json)?\", \"\", txt).strip()\n",
    "    try:\n",
    "        js = json.loads(txt)\n",
    "        return js\n",
    "    except Exception:\n",
    "        logger.warning(\"Multi-metric parse failed: %r\", txt)\n",
    "        return {m: {\"winner\": \"0\", \"reason\": \"Parse failed\"} for m in metrics}\n",
    "\n",
    "\n",
    "def format_background(disease, core_genes):\n",
    "    return f\"disease: {disease}, core_genes: {', '.join(core_genes)}\"\n",
    "def get_first_hypothesis(item):\n",
    "    hypos = item.get(\"hypotheses\", [])\n",
    "    if isinstance(hypos, list) and hypos:\n",
    "        return hypos[0]\n",
    "    elif isinstance(hypos, str) and hypos.strip():\n",
    "        return hypos.strip()\n",
    "    else:\n",
    "        return \"\"\n",
    "if __name__ == \"__main__\":\n",
    "    input_path_a = r\"../data/data_raw/raw_baseline.jsonl\"\n",
    "    input_path_b = r\"../data/data_raw/raw_Multiagent.jsonl\"\n",
    "    output_path = \"baseline_vs_Multiagent.jsonl\"\n",
    "\n",
    "    with open(input_path_a, \"r\", encoding=\"utf-8\") as f_a, \\\n",
    "         open(input_path_b, \"r\", encoding=\"utf-8\") as f_b, \\\n",
    "         open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for idx, (line_a, line_b) in enumerate(zip(f_a, f_b), 1):\n",
    "\n",
    "            item_a = json.loads(line_a)\n",
    "            item_b = json.loads(line_b)\n",
    "            disease = item_a.get(\"disease\", \"\")\n",
    "            core_genes = item_a.get(\"core_genes\", [])\n",
    "            bg = format_background(disease, core_genes)\n",
    "            hypo_a = get_first_hypothesis(item_a)\n",
    "            hypo_b = get_first_hypothesis(item_b)\n",
    "\n",
    "\n",
    "            all_metric_result = call_all_metrics(bg, hypo_a, hypo_b, metrics, metric_defs)\n",
    "\n",
    "            winner_list = [all_metric_result[m][\"winner\"] for m in metrics]\n",
    "            print(f\"Pair {idx}:\", winner_list)\n",
    "            fout.write(json.dumps(winner_list, ensure_ascii=False) + \"\\n\")\n",
    "            fout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
